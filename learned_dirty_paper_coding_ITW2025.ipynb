{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code definitions"
      ],
      "metadata": {
        "id": "W9ct6--c_JXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from flax import linen as nn\n",
        "from flax.training import train_state\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from jax import random\n",
        "import optax\n",
        "\n",
        "import numpy as np\m",
        "from scipy import special\n",
        "\n",
        "from google import colab\n",
        "import os\n",
        "from argparse import Namespace\n",
        "from collections import defaultdict\n",
        "from typing import Callable\n",
        "\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfpd = tfp.distributions\n",
        "\n",
        "\n",
        "jax.config.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VboeEquG3dyF"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  \"\"\"Used for neural encoder and decoder TODO: add reference to paper\"\"\"\n",
        "  output_dims: int\n",
        "  activation_fn: Callable\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, *x):\n",
        "    x = jnp.concatenate(x, -1)\n",
        "    x = nn.Dense(128, name=\"fc1\")(x)\n",
        "    x = self.activation_fn(x)\n",
        "    x = nn.Dense(128, name=\"fc2\")(x)\n",
        "    x = self.activation_fn(x)\n",
        "    x = nn.Dense(self.output_dims, name=\"fc3\")(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(*, logits, labels):\n",
        "  posterior = jax.nn.softmax(logits)\n",
        "  labels_index = jax.nn.one_hot(labels, num_classes=cur_input_message_dims, dtype=bool)\n",
        "  posterior_target = jnp.sum(posterior * labels_index, axis=-1)\n",
        "  cross_entropy = - jnp.log(posterior_target) / jnp.log(2)\n",
        "  return cross_entropy"
      ],
      "metadata": {
        "id": "W8M0Kdldh6Wr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ps3hxStOB9aX"
      },
      "outputs": [],
      "source": [
        "class DirtyPaperCodingModel(nn.Module):\n",
        "  encoder_output_dims: int\n",
        "  input_message_dims: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.encoder = MLP(self.encoder_output_dims, jnp.sin)\n",
        "    self.decoder = MLP(self.input_message_dims, jnp.sin)\n",
        "    # other option for nonlineary is nn.leaky_relu\n",
        "    # can change it to leaky_relu for structured experiments\n",
        "    # more can be found here: https://jax.readthedocs.io/en/latest/jax.nn.html\n",
        "\n",
        "  def __call__(self, input_message, channel_state, channel_noise, seq):\n",
        "    # input message is in the form of modulation points in this case\n",
        "\n",
        "    # encoder with side information (channel state known by the encoder)\n",
        "    x = self.encoder(input_message, channel_state)\n",
        "\n",
        "    # channel output\n",
        "    x_w_noise = x + channel_noise + channel_state # Y = X + S + N, Eq. (2) in the paper\n",
        "\n",
        "    # decoder, carrying out a classification\n",
        "    posterior = self.decoder(x_w_noise)\n",
        "\n",
        "    # encoder output power\n",
        "    power_x = jnp.mean(jnp.sum(jnp.square(x), axis = 1)) # (see Section II.B in the paper)\n",
        "    result = dict(power_x = power_x)\n",
        "\n",
        "    # cross enctropy loss\n",
        "    cross_entropy = cross_entropy_loss(logits=posterior, labels=seq)\n",
        "    cross_entropy = jnp.mean(cross_entropy)\n",
        "\n",
        "    x_out = jnp.argmax(posterior, -1)  # estimated message\n",
        "\n",
        "    accuracy = jnp.mean(x_out == seq)  # 1 - BER\n",
        "\n",
        "    result.update(cross_entropy=cross_entropy, accuracy=accuracy)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling for different modulations and state models\n",
        "\n",
        "def sample_dpc_bpsk(num_samples, var_channel_state, var_channel_noise, input_rng):\n",
        "  # BPSK message\n",
        "  dist_message = tfp.distributions.Categorical(probs=[0.5, 0.5], dtype=int)\n",
        "  modulation_array = jnp.array([-1., 1.])\n",
        "  seq = dist_message.sample(seed=input_rng, sample_shape=(num_samples))\n",
        "  seq = jnp.array(seq)\n",
        "  modulated = jnp.take(modulation_array, seq)\n",
        "  modulated = jnp.expand_dims(jnp.array(modulated), axis=1)\n",
        "  input_message = modulated\n",
        "\n",
        "  # Gaussian state (S) and noise (N)\n",
        "  dist_noise = tfp.distributions.Normal(loc=0., scale=[var_channel_state ** .5, var_channel_noise ** .5])\n",
        "  noises = dist_noise.sample(seed=input_rng, sample_shape=(num_samples, 1))\n",
        "  channel_state, channel_noise = noises[..., 0], noises[..., 1]\n",
        "\n",
        "  return input_message, channel_state, channel_noise, seq\n",
        "\n",
        "\n",
        "def sample_dpc_qpsk(num_samples, var_channel_state, var_channel_noise, input_rng):\n",
        "  # QPSK message\n",
        "  dist_message = tfp.distributions.Categorical(probs=[0.25, 0.25, 0.25, 0.25], dtype=int)\n",
        "  modulation_array = jnp.array([ # QPSK\n",
        "       [0.70710678,  0.70710678],\n",
        "       [ 0.70710678, -0.70710678],\n",
        "       [-0.70710678,  0.70710678],\n",
        "       [-0.70710678, -0.70710678]])\n",
        "  seq = dist_message.sample(seed=input_rng, sample_shape=(num_samples))\n",
        "  seq = jnp.array(seq)\n",
        "\n",
        "  modulated = jnp.take(modulation_array, seq, axis=0)\n",
        "  input_message = modulated\n",
        "\n",
        "  # complex (2d) Gaussian state (S) and noise (N)\n",
        "  dist_noise = tfp.distributions.Normal(loc=0., scale=[(var_channel_state/2) ** .5, (var_channel_noise/2) ** .5]) # because of complex noise, dividing variance by 2 now.\n",
        "  noises = dist_noise.sample(seed=input_rng, sample_shape=(num_samples, 2)) # complex noise\n",
        "  channel_state, channel_noise = noises[..., 0], noises[..., 1]\n",
        "\n",
        "  return input_message, channel_state, channel_noise, seq\n",
        "\n",
        "def sample_dpc_bpsk_structured(num_samples, var_channel_state, var_channel_noise, input_rng):\n",
        "  # BPSK message\n",
        "  message_rng, channel_state_rng = random.split(input_rng)\n",
        "  modulation_array = jnp.array([-1., 1.]) # BPSK\n",
        "\n",
        "  dist_message = tfp.distributions.Categorical(probs=[0.5, 0.5], dtype=int)\n",
        "  seq = dist_message.sample(seed=message_rng, sample_shape=(num_samples))\n",
        "  seq = jnp.array(seq)\n",
        "  modulated = jnp.take(modulation_array, seq)\n",
        "\n",
        "  modulated = jnp.expand_dims(modulated, axis=1)\n",
        "  input_message = modulated\n",
        "\n",
        "  # BPSK state (S)\n",
        "  dist_channel_state = tfp.distributions.Categorical(probs=[0.5, 0.5], dtype=int)\n",
        "  channel_state = dist_channel_state.sample(seed=channel_state_rng, sample_shape=(num_samples))\n",
        "  channel_state = jnp.take(modulation_array, channel_state) * jnp.sqrt(var_channel_state)\n",
        "  channel_state = jnp.expand_dims(channel_state, axis=1)\n",
        "\n",
        "  # Gaussian noise (N)\n",
        "  dist_noise = tfp.distributions.Normal(loc=0., scale=var_channel_noise ** .5)\n",
        "  channel_noise = dist_noise.sample(seed=channel_state_rng, sample_shape=(num_samples, 1))\n",
        "\n",
        "\n",
        "  return input_message, channel_state, channel_noise, seq\n",
        "\n",
        "def sample_dpc_qpsk_structured(num_samples, var_channel_state, var_channel_noise, input_rng):\n",
        "  # QPSK message\n",
        "  message_rng, channel_state_rng = random.split(input_rng)\n",
        "  modulation_array = jnp.array([ # QPSK\n",
        "       [0.70710678,  0.70710678],\n",
        "       [ 0.70710678, -0.70710678],\n",
        "       [-0.70710678,  0.70710678],\n",
        "       [-0.70710678, -0.70710678]])\n",
        "\n",
        "  dist_message = tfp.distributions.Categorical(probs=[0.25, 0.25, 0.25, 0.25], dtype=int)\n",
        "  seq = dist_message.sample(seed=message_rng, sample_shape=(num_samples))\n",
        "  seq = jnp.array(seq)\n",
        "\n",
        "  modulated = jnp.take(modulation_array, seq, axis=0)\n",
        "  input_message = modulated\n",
        "\n",
        "  # QPSK state (S)\n",
        "  dist_channel_state = tfp.distributions.Categorical(probs=[0.25, 0.25, 0.25, 0.25], dtype=int)\n",
        "  channel_state = dist_channel_state.sample(seed=channel_state_rng, sample_shape=(num_samples))\n",
        "  channel_state = jnp.take(modulation_array, channel_state, axis=0) * jnp.sqrt(var_channel_state)\n",
        "\n",
        "  # complex Gaussian (2d) noise (N)\n",
        "  dist_noise = tfp.distributions.Normal(loc=0., scale=(var_channel_noise/2) ** .5) # because of complex noise, dividing variance by 2 now.\n",
        "  channel_noise = dist_noise.sample(seed=input_rng, sample_shape=(num_samples, 2)) # complex noise\n",
        "\n",
        "  return input_message, channel_state, channel_noise, seq\n",
        "\n",
        "\n",
        "def sample_inputs(num_samples, var_channel_state, var_channel_noise, input_rng):\n",
        "  if modulation_type == \"bpsk\":\n",
        "    return sample_dpc_bpsk(num_samples, var_channel_state, var_channel_noise, input_rng)\n",
        "  elif modulation_type == \"qpsk\":\n",
        "    return sample_dpc_qpsk(num_samples, var_channel_state, var_channel_noise, input_rng)\n",
        "  elif modulation_type == \"bpsk_structured\":\n",
        "    return sample_dpc_bpsk_structured(num_samples, var_channel_state, var_channel_noise, input_rng)\n",
        "  elif modulation_type == \"qpsk_structured\":\n",
        "    return sample_dpc_qpsk_structured(num_samples, var_channel_state, var_channel_noise, input_rng)\n",
        "  assert False"
      ],
      "metadata": {
        "id": "-X7G004HQta1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def train_step(state, data_rng, lmbda):\n",
        "  input_message, channel_state, channel_noise, seq = sample_inputs(batch_size, var_channel_state, var_channel_noise, data_rng)\n",
        "  def loss_fn(params):\n",
        "    result = model().apply(\n",
        "        {'params': params}, input_message=input_message,\n",
        "        channel_state=channel_state, channel_noise=channel_noise, seq=seq)\n",
        "    r = Namespace(**result)\n",
        "\n",
        "    loss = r.power_x + lmbda * r.cross_entropy # see Eq. (10) in the paper\n",
        "\n",
        "    result.update(lmbda=lmbda, loss=loss)\n",
        "    return loss, result\n",
        "  grads, aux = jax.grad(loss_fn, has_aux=True)(state.params)\n",
        "  return state.apply_gradients(grads=grads), aux"
      ],
      "metadata": {
        "id": "JV6zPjEZWVKK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(state, input_message, channel_state, channel_noise, seq, lmbda):\n",
        "  result = model().apply(\n",
        "        {'params': state.params}, input_message=input_message,\n",
        "        channel_state=channel_state, channel_noise=channel_noise, seq=seq)\n",
        "  r = Namespace(**result)\n",
        "\n",
        "  loss = r.power_x + lmbda * r.cross_entropy # see Eq. (10) in the paper\n",
        "\n",
        "  return dict(val_power_x=r.power_x, val_loss=loss,\n",
        "              val_cross_entropy=r.cross_entropy, val_accuracy=r.accuracy)\n"
      ],
      "metadata": {
        "id": "vE7Jhzjlm0P6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "  _, axs = plt.subplots(1, 4, figsize=(25, 5))\n",
        "\n",
        "  for k in [\"power_x\", \"val_power_x\"]:\n",
        "    axs[0].plot(history[k])\n",
        "  axs[0].legend([\"train\", \"val\"], loc=\"best\")\n",
        "  axs[0].grid()\n",
        "  axs[0].set_title(\"avg. power\")\n",
        "\n",
        "  for k in [\"cross_entropy\", \"val_cross_entropy\"]:\n",
        "    axs[1].plot(history[k])\n",
        "  axs[1].legend([\"train\", \"val\"], loc=\"best\")\n",
        "  axs[1].grid()\n",
        "  axs[1].set_title(\"cross_entropy\")\n",
        "  axs[1].set_yscale(\"log\")\n",
        "\n",
        "  for k in [\"accuracy\", \"val_accuracy\"]:\n",
        "    axs[2].plot(history[k])\n",
        "  axs[2].legend([\"train\", \"val\"], loc=\"best\")\n",
        "  axs[2].grid()\n",
        "  axs[2].set_title(\"accuracy\")\n",
        "  axs[2].set_yscale(\"log\")\n",
        "\n",
        "  for k in [\"loss\", \"val_loss\"]:\n",
        "    axs[3].plot(history[k])\n",
        "  axs[3].legend([\"train\", \"val\"], loc=\"best\")\n",
        "  axs[3].grid()\n",
        "  axs[3].set_title(\"loss\")\n",
        "  axs[3].set_yscale(\"log\")\n"
      ],
      "metadata": {
        "id": "Epc7lJ2eY0HN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ser_bounds(power_x, snr_range = [-4.0,  12.0]):\n",
        "  # ser vs snr lower bounds for different modulation schemes for AWGN channel\n",
        "  N_pts = 100\n",
        "  snr_vals_db = np.linspace(snr_range[0], snr_range[1], N_pts)\n",
        "  snr_vals = 10.0 ** (snr_vals_db / 10)\n",
        "\n",
        "  if modulation_type == \"bpsk\" or modulation_type == \"bpsk_structured\":\n",
        "\n",
        "    ser_lb = 0.5 - 0.5*special.erf(np.sqrt(snr_vals/2))\n",
        "    ser_lb_db = np.log10(ser_lb)\n",
        "\n",
        "  elif modulation_type == \"qpsk\" or modulation_type == \"qpsk_structured\":\n",
        "\n",
        "    ser_lb = 2*(0.5 - 0.5*special.erf(np.sqrt(snr_vals/2)))\n",
        "    ser_lb_db = np.log10(ser_lb)\n",
        "\n",
        "  return ser_lb_db, snr_vals_db\n",
        "\n",
        "\n",
        "def plot_bounds(power_x, var_channel_state,  snr_range = [-4.0, 12.0]):\n",
        "\n",
        "  ser_lb_db, snr_vals_db = ser_bounds(power_x, snr_range)\n",
        "\n",
        "  plt.plot(snr_vals_db, ser_lb_db, label = \"Lower bound (AWGN)\")\n",
        "\n",
        "# ser vs snr performance of Tomlinson-Harashima precoder based schemes\n",
        "th_bpsk_ser = np.array([-0.302, -0.307, -0.313, -0.323, -0.337, -0.356, -0.382, -0.414, -0.452, -0.498, -0.552, -0.613, -0.684, -0.766, -0.856, -0.961, -1.083, -1.218, -1.372, -1.558, -1.76, -2.01, -2.289, -2.6, -2.971, -3.427, -3.899, -4.521, -5.142])\n",
        "th_bpsk_snr = np.array([-5,\t-4.31,\t-3.62,\t-2.931,\t-2.241,\t-1.551,\t-0.862, -0.172,\t0.5172,\t1.2068,\t1.8965,\t2.5862,\t3.2758,\t3.9655,\t4.6551,\t5.3448,\t6.0344,\t6.7241,\t7.4137,\t8.1034,\t8.7931,\t9.4827,\t10.172,\t10.862,\t11.551,\t12.241,\t12.931,\t13.62,\t14.31])\n",
        "\n",
        "\n",
        "th_qpsk_ser = np.array([-0.1268, -0.1292, -0.1334, -0.1404, -0.1498, -0.1637, -0.1823, -0.2058, -0.2365, -0.2722, -0.3165, -0.3700, -0.4313, -0.5038, -0.588, -0.6856, -0.7994, -0.9312, -1.0846, -1.2634, -1.4699, -1.7053, -1.9842, -2.3171, -2.6868, -3.1135, -3.6091, -4.2534, -5.664])\n",
        "th_qpsk_snr = np.array([-5,\t-4.31,\t-3.62,\t-2.931,\t-2.241,\t-1.551,\t-0.862, -0.172,\t0.5172,\t1.2068,\t1.8965,\t2.5862,\t3.2758,\t3.9655,\t4.6551,\t5.3448,\t6.0344,\t6.7241,\t7.4137,\t8.1034,\t8.7931,\t9.4827,\t10.172,\t10.862,\t11.551,\t12.241,\t12.931,\t13.62,\t14.31])"
      ],
      "metadata": {
        "id": "NNXgz_jsdICx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments\n"
      ],
      "metadata": {
        "id": "XI1O6JGN_QUg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM4yiWiL28de"
      },
      "outputs": [],
      "source": [
        "lmbda = 5.5\n",
        "var_channel_state, var_channel_noise = 30.0, 1.0\n",
        "# modulation type options: bpsk, qpsk, bpsk_structured, qpsk_structured\n",
        "modulation_type = \"qpsk\"\n",
        "\n",
        "if modulation_type == \"bpsk\" or modulation_type == \"bpsk_structured\":\n",
        "  entropy_message = 1. # equally likely symbols\n",
        "  cur_input_message_dims = 2\n",
        "  encoder_output_dims = 1\n",
        "elif modulation_type == \"qpsk\" or modulation_type == \"qpsk_structured\":\n",
        "  entropy_message = 2. # equally likely symbols\n",
        "  cur_input_message_dims = 4\n",
        "  encoder_output_dims = 2\n",
        "else:\n",
        "  assert False # not defined yet.\n",
        "\n",
        "print('lmbda is', lmbda)\n",
        "print('modulation type is', modulation_type)\n",
        "print('variance for channel state is', var_channel_state, 'and variance for channel noise is', var_channel_noise)\n",
        "\n",
        "init_samples = 10\n",
        "batch_size = 1024\n",
        "num_epochs = 150 * 4\n",
        "steps_per_epoch = 1000\n",
        "validation_size = 60 * 1024\n",
        "test_steps = 1024\n",
        "\n",
        "total_steps = num_epochs * steps_per_epoch\n",
        "\n",
        "lr_schedule = optax.piecewise_constant_schedule(\n",
        "    init_value=1e-4,\n",
        "    boundaries_and_scales={\n",
        "        int(2/4 * total_steps): 1e-1,\n",
        "        int(3/4 * total_steps): 1e-1,\n",
        "    },\n",
        ")\n",
        "\n",
        "def model():\n",
        "  return DirtyPaperCodingModel(input_message_dims=cur_input_message_dims, encoder_output_dims=encoder_output_dims)\n",
        "\n",
        "seed, = np.frombuffer(os.getrandom(4), dtype=np.int32)\n",
        "rng = random.PRNGKey(seed)\n",
        "\n",
        "rng, init_rng, init_data_rng = random.split(rng, 3)\n",
        "init_input_message, init_channel_state, init_channel_noise, init_seq = sample_inputs(\n",
        "              init_samples, var_channel_state, var_channel_noise, init_data_rng)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn=model().apply,\n",
        "    params=model().init(init_rng,\n",
        "                        input_message=init_input_message,\n",
        "                        channel_state=init_channel_state,\n",
        "                        channel_noise=init_channel_noise,\n",
        "                        seq=init_seq\n",
        "       )['params'],\n",
        "    tx=optax.adam(lr_schedule),\n",
        ")\n",
        "\n",
        "rng, data_rng = random.split(rng)\n",
        "validation_set = sample_inputs(validation_size, var_channel_state, var_channel_noise, data_rng)\n",
        "\n",
        "history = defaultdict(lambda: np.full((num_epochs,), float(\"nan\")))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  results = defaultdict(lambda: 0, lr=lr_schedule(state.step))\n",
        "\n",
        "  for _ in range(steps_per_epoch):\n",
        "    rng, data_rng = random.split(rng)\n",
        "    state, result = train_step(\n",
        "          state, data_rng=data_rng, lmbda=lmbda)\n",
        "    for k in result:\n",
        "      results[k] += result[k]\n",
        "  for k in results:\n",
        "    results[k] /= steps_per_epoch\n",
        "\n",
        "  results.update(eval_step(state, *validation_set, lmbda=lmbda))\n",
        "\n",
        "  for k in results:\n",
        "    history[k][epoch] = results[k]\n",
        "\n",
        "  colab.output.clear(wait=True)\n",
        "  plot_history(history)\n",
        "  plt.show()\n",
        "  print(f\"epoch {epoch:4}\")\n",
        "  print(f\"train      (avg.) power {history['power_x'][epoch]:6.3f}, cross-entropy {history['cross_entropy'][epoch]:7.4f}, accuracy {history['accuracy'][epoch]:7.4f}, loss {history['loss'][epoch]:7.4f}\")\n",
        "  print(f\"validation (avg.) power {history['val_power_x'][epoch]:6.3f}, cross-entropy {history['val_cross_entropy'][epoch]:7.4f}, accuracy {history['val_accuracy'][epoch]:7.4f}, loss {history['val_loss'][epoch]:7.4f}\", flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference for matching train-test var_channel_state\n",
        "metrics = defaultdict(lambda: 0)\n",
        "for _ in range(test_steps):\n",
        "  rng, data_rng = random.split(rng)\n",
        "  result = eval_step(state, *sample_inputs(batch_size, var_channel_state, var_channel_noise, data_rng), lmbda=lmbda)\n",
        "  for k in result:\n",
        "    metrics[k] += result[k]\n",
        "for k in metrics:\n",
        "  metrics[k] /= test_steps\n",
        "print(f\"Test (avg.) power {metrics['val_power_x']:6.6f}, cross-entropy {metrics['val_cross_entropy']:7.6f}, accuracy {metrics['val_accuracy']:7.6f}, loss {metrics['val_loss']:6.6f}\")\n",
        "\n",
        "\n",
        "print(f\"pgfplots: avg. power {metrics['val_power_x']:11.10f}, bit error rate  {1 - metrics['val_accuracy']:11.10f}, Capacity-LowerBound {entropy_message-metrics['val_cross_entropy']:11.10f}, for lmbda = {lmbda}\")\n",
        "print(f\"[dB] pgfplots: avg. power {10*np.log10(metrics['val_power_x']/var_channel_noise):11.10f}, bit error rate  {np.log10(1 - metrics['val_accuracy']):11.10f}, Capacity-LowerBound {entropy_message-metrics['val_cross_entropy']:11.10f}, for lmbda = {lmbda}\")"
      ],
      "metadata": {
        "id": "OB-9QEXKB1GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting"
      ],
      "metadata": {
        "id": "CyLo2VllSKvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison with lower and upper bounds\n",
        "\n",
        "plot_bounds(metrics['val_power_x'], var_channel_state,  snr_range = [-2.0, 8.0])\n",
        "\n",
        "if modulation_type == \"bpsk\" or modulation_type == \"bpsk_structured\":\n",
        "  plt.plot(th_bpsk_snr, th_bpsk_ser, label=\"Tomlinson-Harashima BPSK\")\n",
        "elif modulation_type == \"qpsk\" or modulation_type == \"qpsk_structured\":\n",
        "  plt.plot(th_bpsk_snr, th_qpsk_ser, label=\"Tomlinson-Harashima BPSK\")\n",
        "\n",
        "\n",
        "plt.plot(10*np.log10(metrics['val_power_x']/var_channel_noise) , np.log10(1 - metrics['val_accuracy']), 'ro', label=f\"Result w/ channel state var : {var_channel_state}\")\n",
        "print(f\"[dB] pgfplots: avg. power {10*np.log10(metrics['val_power_x']/var_channel_noise):11.10f}, bit error rate  {np.log10(1 - metrics['val_accuracy']):11.10f} for lmbda = {lmbda}\")\n",
        "\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\" log10 (BER) \")\n",
        "\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "plt.minorticks_on()\n",
        "plt.ylim(-2.0, 0)\n",
        "plt.xlim(-2.0,8.0)\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TqLyfEBqzGqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder/Decoder Behaviour"
      ],
      "metadata": {
        "id": "hkzI_4MbUz9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def encoder_behavior(state, input_message, channel_state):\n",
        "  return model().apply(\n",
        "      {'params': state.params}, method = lambda m: m.encoder(input_message, channel_state))\n",
        "\n",
        "@jax.jit\n",
        "def decoder_behavior(state, input_w_noise):\n",
        "  return model().apply(\n",
        "      {'params': state.params}, method = lambda m: m.decoder(input_w_noise))\n"
      ],
      "metadata": {
        "id": "b1MPw23soBbA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_THP_encoder_bpsk(channel_state, metrics):\n",
        "  # BPSK encoder output vs channel state for THP, see Eq. (3) in the paper\n",
        "  avg_power = metrics['val_power_x']\n",
        "  modulo_width = np.sqrt(avg_power * 12)\n",
        "\n",
        "  th_encoder_output = np.mod(-channel_state + modulo_width/2, modulo_width) - modulo_width/2;\n",
        "\n",
        "  plt.plot(channel_state, th_encoder_output, '-k', label=f\"TH Encoder w/ same avg. power\")\n",
        "\n",
        "def plot_encoding_bpsk(state, input_symbols, channel_state_lim = 15.):\n",
        "  # Plot and compare encoder output vs channel state for learned model and THP\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  channel_state = np.linspace(-channel_state_lim, channel_state_lim, 700)[:, None]\n",
        "\n",
        "  for input_symbol in input_symbols:\n",
        "    input_message = np.full(channel_state.shape, input_symbol)\n",
        "    encoder_output = encoder_behavior(state, input_message, channel_state)\n",
        "    plt.plot(channel_state, encoder_output, marker='o', linestyle='-', label=f\"Input Symbol: {input_symbol}\")\n",
        "\n",
        "  plot_THP_encoder_bpsk(channel_state, metrics)\n",
        "\n",
        "  plt.xlabel(\"Channel State\")\n",
        "  plt.ylabel(\"Encoder Output\")\n",
        "  plt.title(\"Encoder Output for Different Input Symbols and Channel States\")\n",
        "\n",
        "  # More precise grid\n",
        "  plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "  plt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "  plt.minorticks_on()\n",
        "\n",
        "  #plt.ylim(-.5, .5) # change the boundaries accordingly\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def plot_decoding_bpsk(state):\n",
        "  num_samples = 100 * 2\n",
        "  plt.figure(figsize=(10, 2))\n",
        "  channel_output = np.linspace(-15, 15, num_samples)[:, None]\n",
        "\n",
        "  decoded_output = decoder_behavior(state, channel_output)\n",
        "  estimated_message = jnp.argmax(decoded_output, -1)  # estimated message\n",
        "\n",
        "  plt.plot(channel_output, estimated_message)\n",
        "\n",
        "  plt.xlabel(\"Channel Output\")\n",
        "  plt.ylabel(\"[Hard] Decoder Output\")\n",
        "  plt.title(\"[Hard] Decoder Output for Different Input Symbols and Channel States\")\n",
        "\n",
        "  # More precise grid\n",
        "  plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "  plt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "  plt.minorticks_on()\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def plot_decoding_qpsk(state, channel_state_lim = 16):\n",
        "    channel_states = np.linspace(-channel_state_lim, channel_state_lim, 200)\n",
        "    X, Y = np.meshgrid(channel_states, channel_states)\n",
        "    channel_state = np.stack((X.flatten(), Y.flatten()), axis=-1)\n",
        "\n",
        "    # Assuming decoder_behavior takes a 2D input (real and imaginary parts)\n",
        "    decoded_output = decoder_behavior(state, channel_state)\n",
        "    estimated_message = jnp.argmax(decoded_output, axis=-1)\n",
        "\n",
        "    # Reshape the estimated message to match the meshgrid shape\n",
        "    Z = estimated_message.reshape(X.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(Z, extent=[channel_states.min(), channel_states.max(), channel_states.min(), channel_states.max()], origin='lower', cmap='viridis', vmin=0, vmax=3) #Set vmin and vmax for 4 discrete classes\n",
        "    plt.xlabel(\"Channel Output (Real)\")\n",
        "    plt.ylabel(\"Channel Output (Imaginary)\")\n",
        "    plt.title(\"Hard Decoder Decisions (QPSK)\")\n",
        "    plt.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
        "    plt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\n",
        "    plt.minorticks_on()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "YB3zUJBYF40z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if modulation_type == \"bpsk\":\n",
        "  plot_encoding_bpsk(state, input_symbols=[-1, 1]) # input symbols are BPSK ones"
      ],
      "metadata": {
        "id": "zB9rD2EvUDq2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if modulation_type == \"bpsk\":\n",
        "  plot_decoding_bpsk(state) # BPSK\n",
        "elif modulation_type == \"qpsk\":\n",
        "  plot_decoding_qpsk(state)"
      ],
      "metadata": {
        "id": "ktw3pNrUopCy"
      },
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
